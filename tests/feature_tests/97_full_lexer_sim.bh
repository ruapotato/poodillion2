# Full lexer simulation - close to real lexer.py
try:
    from typing import List
except:
    pass

# Many token types
TT_DEF: int = 1
TT_CLASS: int = 2
TT_IF: int = 7
TT_ELSE: int = 9
TT_WHILE: int = 10
TT_RETURN: int = 6
TT_INT: int = 49
TT_IDENT: int = 59
TT_NUMBER: int = 60
TT_STRING: int = 61
TT_FSTRING: int = 62
TT_PLUS: int = 67
TT_MINUS: int = 68
TT_STAR: int = 69
TT_SLASH: int = 70
TT_EQUALS: int = 74
TT_EQUALS_EQUALS: int = 75
TT_COLON: int = 117
TT_LPAREN: int = 105
TT_RPAREN: int = 106
TT_NEWLINE: int = 130
TT_EOF: int = 131
TT_ARROW: int = 126
TT_PLUS_EQUALS: int = 81

def is_alpha(c: int) -> int:
    if c >= 65 and c <= 90:
        return 1
    if c >= 97 and c <= 122:
        return 1
    return 0

def is_digit(c: int) -> int:
    if c >= 48 and c <= 57:
        return 1
    return 0

def is_alnum(c: int) -> int:
    if is_alpha(c) == 1:
        return 1
    if is_digit(c) == 1:
        return 1
    return 0

def str_eq(a: str, b: str) -> int:
    if len(a) != len(b):
        return 0
    i: int = 0
    while i < len(a):
        if a[i] != b[i]:
            return 0
        i = i + 1
    return 1

def lookup_keyword(word: str) -> int:
    if str_eq(word, "def") == 1:
        return TT_DEF
    if str_eq(word, "class") == 1:
        return TT_CLASS
    if str_eq(word, "if") == 1:
        return TT_IF
    if str_eq(word, "else") == 1:
        return TT_ELSE
    if str_eq(word, "while") == 1:
        return TT_WHILE
    if str_eq(word, "return") == 1:
        return TT_RETURN
    if str_eq(word, "int") == 1:
        return TT_INT
    return 0

class Token:
    def __init__(self, ttype: int, value, line: int, column: int, end_line: int = 0, end_column: int = 0):
        self.ttype: int = ttype
        self.value = value
        self.line: int = line
        self.column: int = column
        self.end_line: int = end_line if end_line != 0 else line
        self.end_column: int = end_column if end_column != 0 else column

class Lexer:
    def __init__(self, source: str):
        self.source: str = source
        self.pos: int = 0
        self.line: int = 1
        self.column: int = 1
        self.tokens: List[Token] = []
        self.indent_stack: List[int] = [0]

    def at_end(self) -> bool:
        return self.pos >= len(self.source)

    def current_char(self) -> char:
        return self.source[self.pos]

    def peek_char(self, offset: int = 1) -> char:
        pos: int = self.pos + offset
        if pos >= len(self.source):
            return ' '
        return self.source[pos]

    def peek_valid(self, offset: int = 1) -> bool:
        return self.pos + offset < len(self.source)

    def advance(self):
        if self.pos < len(self.source):
            ch: char = self.source[self.pos]
            if ch == '\n':
                self.line = self.line + 1
                self.column = 1
            else:
                self.column = self.column + 1
            self.pos = self.pos + 1

    def skip_whitespace(self):
        while not self.at_end():
            ch: char = self.current_char()
            if ch == ' ' or ch == '\t':
                self.advance()
            else:
                break

    def skip_comment(self):
        if self.current_char() == '#':
            while not self.at_end() and self.current_char() != '\n':
                self.advance()

    def read_number(self) -> Token:
        start_line: int = self.line
        start_col: int = self.column
        num: int = 0

        while not self.at_end():
            ch: char = self.current_char()
            if is_digit(ord(ch)) == 1:
                num = num * 10 + (ord(ch) - 48)
                self.advance()
            else:
                break

        return Token(TT_NUMBER, num, start_line, start_col, self.line, self.column)

    def read_string(self, is_fstring: bool = False) -> Token:
        start_line: int = self.line
        start_col: int = self.column
        quote: char = self.current_char()
        self.advance()

        string: str = ""
        while not self.at_end():
            ch: char = self.current_char()
            if ch == quote:
                self.advance()
                break
            if ch == '\\':
                self.advance()
                esc: char = self.current_char()
                if esc == 'n':
                    string = string + '\n'
                elif esc == 't':
                    string = string + '\t'
                else:
                    string = string + esc
                self.advance()
            else:
                string = string + ch
                self.advance()

        ttype: int = TT_FSTRING if is_fstring else TT_STRING
        return Token(ttype, string, start_line, start_col, self.line, self.column)

    def read_identifier(self) -> Token:
        print("read_ident\n")
        start_line: int = self.line
        start_col: int = self.column
        ident: str = ""

        while not self.at_end():
            ch: char = self.current_char()
            if is_alnum(ord(ch)) == 1 or ch == '_':
                ident = ident + ch
                self.advance()
            else:
                break

        print("lookup\n")
        ttype: int = lookup_keyword(ident)
        print("got ttype\n")
        if ttype == 0:
            ttype = TT_IDENT

        value = None if ttype != TT_IDENT else ident
        print("return tok\n")
        return Token(ttype, value, start_line, start_col, self.line, self.column)

    def tokenize(self) -> List[Token]:
        print("tokenize start\n")
        while not self.at_end():
            print("loop\n")
            ch: char = self.current_char()
            print("got ch\n")
            start_line: int = self.line
            print("got start_line\n")
            start_col: int = self.column
            print("got start_col\n")

            # Newlines
            print("check newline\n")
            if ch == '\n':
                self.advance()
                self.tokens.append(Token(TT_NEWLINE, None, start_line, start_col, self.line, self.column))
                continue

            # Skip whitespace
            print("check whitespace\n")
            if ch == ' ' or ch == '\t':
                self.skip_whitespace()
                continue

            # Comments
            print("check comment\n")
            if ch == '#':
                self.skip_comment()
                continue

            # F-strings
            print("check fstring\n")
            pk = self.peek_char()
            print("got pk\n")
            if (ch == 'f' or ch == 'F') and (pk == '"' or pk == "'"):
                self.advance()
                self.tokens.append(self.read_string(is_fstring=True))
                continue

            # Numbers
            if is_digit(ord(ch)) == 1:
                self.tokens.append(self.read_number())
                continue

            # Strings
            if ch == '"' or ch == "'":
                self.tokens.append(self.read_string())
                continue

            # Identifiers and keywords
            if is_alpha(ord(ch)) == 1 or ch == '_':
                print("before read_ident\n")
                self.tokens.append(self.read_identifier())
                print("after read_ident\n")
                continue

            # Operators
            if ch == '+':
                if self.peek_char() == '=':
                    self.advance()
                    self.advance()
                    self.tokens.append(Token(TT_PLUS_EQUALS, None, start_line, start_col, self.line, self.column))
                else:
                    self.advance()
                    self.tokens.append(Token(TT_PLUS, None, start_line, start_col, self.line, self.column))
            elif ch == '-':
                if self.peek_char() == '>':
                    self.advance()
                    self.advance()
                    self.tokens.append(Token(TT_ARROW, None, start_line, start_col, self.line, self.column))
                else:
                    self.advance()
                    self.tokens.append(Token(TT_MINUS, None, start_line, start_col, self.line, self.column))
            elif ch == '*':
                self.advance()
                self.tokens.append(Token(TT_STAR, None, start_line, start_col, self.line, self.column))
            elif ch == '/':
                self.advance()
                self.tokens.append(Token(TT_SLASH, None, start_line, start_col, self.line, self.column))
            elif ch == '=':
                if self.peek_char() == '=':
                    self.advance()
                    self.advance()
                    self.tokens.append(Token(TT_EQUALS_EQUALS, None, start_line, start_col, self.line, self.column))
                else:
                    self.advance()
                    self.tokens.append(Token(TT_EQUALS, None, start_line, start_col, self.line, self.column))
            elif ch == ':':
                self.advance()
                self.tokens.append(Token(TT_COLON, None, start_line, start_col, self.line, self.column))
            elif ch == '(':
                self.advance()
                self.tokens.append(Token(TT_LPAREN, None, start_line, start_col, self.line, self.column))
            elif ch == ')':
                self.advance()
                self.tokens.append(Token(TT_RPAREN, None, start_line, start_col, self.line, self.column))
            else:
                self.advance()

        self.tokens.append(Token(TT_EOF, None, self.line, self.column, self.line, self.column))
        return self.tokens

def main() -> int:
    print("main\n")
    code: str = "def foo(x: int) -> int:\n    return x + 1"
    print("create lexer\n")
    lexer: Lexer = Lexer(code)
    print("tokenize\n")
    tokens: List[Token] = lexer.tokenize()
    print("done\n")
    n: int = len(tokens)
    if n >= 15:
        print("OK\n")
    else:
        print("FAIL\n")
    return 0
