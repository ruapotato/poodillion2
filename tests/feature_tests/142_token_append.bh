# Test Token creation and list append in tokenize pattern
class Token:
    kind: int
    value: str
    start_line: int
    start_col: int
    end_line: int
    end_col: int

class Lexer:
    source: str
    pos: int
    line: int
    column: int
    tokens: List[Token]

    def __init__(self, code: str):
        self.source = code
        self.pos = 0
        self.line = 1
        self.column = 1
        self.tokens = []

    def at_end(self) -> int:
        if self.pos >= len(self.source):
            return 1
        return 0

    def current_char(self) -> char:
        return self.source[self.pos]

    def advance(self) -> void:
        self.pos = self.pos + 1
        self.column = self.column + 1

    def tokenize(self) -> List[Token]:
        while self.at_end() == 0:
            ch: char = self.current_char()
            start_line: int = self.line
            start_col: int = self.column

            # Create token for each character
            tok: Token = Token()
            tok.kind = ord(ch)
            tok.value = ""
            tok.start_line = start_line
            tok.start_col = start_col
            tok.end_line = self.line
            tok.end_col = self.column + 1
            self.tokens.append(tok)

            self.advance()

        return self.tokens

def main() -> int:
    print("Creating\n")
    lexer: Lexer = Lexer("abc")
    print("Tokenizing\n")
    tokens: List[Token] = lexer.tokenize()
    print("Done\n")
    n: int = len(tokens)
    if n == 3:
        print("OK\n")
    else:
        print("FAIL\n")
    return 0
