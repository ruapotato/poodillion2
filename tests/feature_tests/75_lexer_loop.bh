# Test 75: Simple Lexer tokenize loop
try:
    from typing import List
except:
    pass

class Token:
    def __init__(self, ttype: int, value: str):
        self.ttype: int = ttype
        self.value: str = value

class Lexer:
    def __init__(self, source: str):
        self.source: str = source
        self.pos: int = 0
        self.tokens: List[Token] = []

    def at_end(self) -> bool:
        return self.pos >= len(self.source)

    def current_char(self) -> char:
        return self.source[self.pos]

    def advance(self):
        self.pos = self.pos + 1

    def tokenize(self) -> List[Token]:
        while not self.at_end():
            ch: char = self.current_char()
            print("Char: ")
            print(ch)
            print("\n")

            # Create a simple token for each char
            tok: Token = Token(1, "char")
            self.tokens.append(tok)
            self.advance()

        return self.tokens

def main() -> int:
    print("Lexer loop test\n")

    code: str = "abc"
    print("Creating lexer\n")
    lexer: Lexer = Lexer(code)
    print("Tokenizing\n")
    tokens: List[Token] = lexer.tokenize()
    print("Done\n")

    n: int = len(tokens)
    if n == 3:
        print("Count: OK\n")
    else:
        print("Count: FAIL\n")

    return 0
