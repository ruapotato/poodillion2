# Test lexer tokenize with identifier - mimics the exact code flow
class Token:
    kind: int
    value: str
    start_line: int
    start_col: int
    end_line: int
    end_col: int

def is_alnum(c: int) -> int:
    if c >= 48 and c <= 57:
        return 1
    if c >= 65 and c <= 90:
        return 1
    if c >= 97 and c <= 122:
        return 1
    return 0

class Lexer:
    source: str
    pos: int
    line: int
    column: int
    tokens: List[Token]

    def init(self, code: str) -> Lexer:
        self.source = code
        self.pos = 0
        self.line = 1
        self.column = 1
        self.tokens = []
        return self

    def at_end(self) -> int:
        if self.pos >= len(self.source):
            return 1
        return 0

    def current_char(self) -> char:
        return self.source[self.pos]

    def advance(self) -> void:
        self.pos = self.pos + 1
        self.column = self.column + 1

    def read_identifier(self) -> Token:
        start_line: int = self.line
        start_col: int = self.column
        ident: str = ""

        while self.at_end() == 0:
            ch: char = self.current_char()
            if is_alnum(ord(ch)) == 1 or ch == '_':
                ident = ident + ch
                self.advance()
            else:
                break

        tok: Token = Token()
        tok.kind = 1
        tok.value = ident
        tok.start_line = start_line
        tok.start_col = start_col
        tok.end_line = self.line
        tok.end_col = self.column
        return tok

    def tokenize(self) -> List[Token]:
        while self.at_end() == 0:
            ch: char = self.current_char()

            # Check for identifier start
            if is_alnum(ord(ch)) == 1 or ch == '_':
                tok: Token = self.read_identifier()
                self.tokens.append(tok)
                continue

            # Skip unknown
            self.advance()

        return self.tokens

def main() -> int:
    print("Creating lexer\n")
    lexer: Lexer = Lexer().init("x")
    print("Tokenizing\n")
    tokens: List[Token] = lexer.tokenize()
    print("Done\n")

    n: int = len(tokens)
    if n == 1:
        print("OK\n")
    else:
        print("FAIL\n")
    return 0
