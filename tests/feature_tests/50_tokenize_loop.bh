# Test 50: Tokenize loop pattern (mimics lexer.tokenize())
try:
    from typing import List
except:
    pass

class Token:
    def __init__(self, kind: int, line: int):
        self.kind: int = kind
        self.line: int = line

class Lexer:
    def __init__(self, text: str):
        self.source: str = text
        self.pos: int = 0
        self.line: int = 1
        self.tokens: List[Token] = []

    def at_end(self) -> bool:
        n: int = len(self.source)
        return self.pos >= n

    def current_char(self) -> char:
        return self.source[self.pos]

    def advance(self) -> char:
        ch: char = self.source[self.pos]
        self.pos = self.pos + 1
        return ch

    def skip_whitespace(self):
        while not self.at_end():
            ch: char = self.current_char()
            if ch == ' ' or ch == '\t':
                self.advance()
            elif ch == '\n':
                self.line = self.line + 1
                self.advance()
            else:
                break

    def add_token(self, kind: int):
        t: Token = Token(kind, self.line)
        self.tokens.append(t)

    def get_count(self) -> int:
        return len(self.tokens)

    def tokenize(self):
        while not self.at_end():
            self.skip_whitespace()
            if self.at_end():
                break

            ch: char = self.current_char()
            self.add_token(1)
            self.advance()

def main() -> int:
    print("Tokenize loop test\n")

    lex: Lexer = Lexer("a b c")
    lex.tokenize()

    n: int = lex.get_count()
    if n == 3:
        print("Count: OK\n")
    else:
        print("Count: FAIL\n")

    print("Done\n")
    return 0
